Best Practices in Prompt Engineering for AI-Assisted Brainstorming
Effective prompt engineering can dramatically improve the quality of AI-generated ideas. Drawing on insights from experts like Gwern Branwen, Janus, and Anthropic’s prompt guides, we outline key principles and new strategies for crafting a meta prompt – a general prompt template – optimized for brainstorming. We then propose three refined meta prompt structures, explain when to use each, and show how to adapt the prompt (keeping ~70% general structure and ~30% domain specifics) for domains like business strategy, creative writing, and scientific research.
Key Principles from Prompt Engineering Experts
Clarity and Context: Always begin by clearly describing the task and providing any necessary context. Anthropic recommends treating the AI as “an intern on their first day” – give explicit instructions and relevant detail so it knows exactly what you need (Prompt engineering for business performance \ Anthropic). A well-specified prompt avoids ambiguity and guides the model toward the right focus.

“Prompts as Programming”: Gwern Branwen emphasizes that prompt writing is an iterative, experimental process much like coding (3 Prompt Engineering Lessons You Must Know). A single prompt failure often means the prompt was at fault, not the model – “GPT-3 may ‘fail’ if a prompt is poorly-written... The question is not whether a given prompt works, but whether any prompt works” (3 Prompt Engineering Lessons You Must Know). In practice, this means we should test and refine prompts repeatedly (debugging them) until the AI produces the desired output.

Iterative Refinement: Don’t expect the first prompt to be perfect. Gwern and others stress trying different phrasings, formats, or decompositions of the problem to find what the model responds to best (Prompt Engineering Guide for UX/UI Designers | UXPin). For example, if a direct question yields poor results, you might rephrase it as a dialogue or break one task into smaller steps (Prompt Engineering Guide for UX/UI Designers | UXPin). Continually observe and tweak the prompt based on the AI’s output.

Human-Like Framing: Make prompts relatable by using natural, narrative cues. Gwern suggests framing the request in a more human or story-like manner when possible (Prompt Engineering Guide for UX/UI Designers | UXPin). Setting a scene or scenario can activate relevant patterns in the model. For instance, instead of a terse command, you might say “Imagine you are a wise innovation coach guiding a brainstorming session on X…”. This leverages the model’s training on human-like text to elicit more nuanced and engaging responses.

Persona and Role Play: Giving the AI a clear role or persona often improves consistency and quality. Janus (in the Waluigi Effect essay) describes a common prompt pattern where first the AI is cast as a character with certain desirable traits, then the user’s query is asked (The Waluigi Effect (mega-post) - LessWrong 2.0 viewer). For example, “Alice is a smart, honest, helpful assistant… Bob asks: [question]… Alice answers:” tends to yield more correct and relevant answers than a raw question, because the model is primed to “act” as a knowledgeable, helpful entity (The Waluigi Effect (mega-post) - LessWrong 2.0 viewer). In practice, assigning a role (“You are an expert in X…”) sets context and tone, and the model will try to fulfill that role in its answer (Prompt Engineering Guide for UX/UI Designers | UXPin).

Few-Shot Examples: Provide examples in the prompt to guide the AI’s style and format. This technique, known as few-shot prompting, is cited by experts like Rachel Thomas as crucial for getting reliable output (Prompt Engineering Guide for UX/UI Designers | UXPin). By showing the model a couple of input→output examples or a sample solution, you help it infer the pattern you want. The examples should be realistic and specific, even covering edge cases (Prompt engineering for business performance \ Anthropic), so the AI knows exactly what a correct answer looks like.

Step-by-Step Reasoning (Chain-of-Thought): Encourage the AI to think stepwise for complex tasks. Anthropic notes that simply instructing the model to “think step by step” often improves accuracy on reasoning problems (Prompt engineering for business performance \ Anthropic). This taps into the chain-of-thought approach, where the AI is guided to break down its reasoning or enumerate ideas in a logical sequence. You can explicitly prompt this by phrases like “Let’s consider this step by step,” or even use scratchpad markers (e.g. <thinking> tags) as in Anthropic’s examples (Prompt engineering for business performance \ Anthropic) (Prompt engineering for business performance \ Anthropic). The result is more coherent and thorough brainstorming, as the model is less likely to skip important steps.

Divide and Conquer: If the query is multifaceted, break it into sub-prompts or stages. Both researchers and practitioners have found that some tasks are too difficult for a single pass, but solvable if broken into parts (Methods of prompt programming :: — Moire). This can be done in one prompt (by instructing the AI to handle one aspect at a time) or via prompt chaining (feeding the output of one step into the next). Anthropic’s guide calls this prompt chaining, useful when tackling complex workflows (Prompt engineering for business performance \ Anthropic). For example, first ask for a list of ideas, then in a second prompt ask the AI to expand on or evaluate those ideas. This staged approach keeps the model focused and mitigates confusion.

Constraints and “Don’t Know” Options: It’s often helpful to set boundaries or let the AI know what to avoid. For instance, to reduce hallucinations or off-topic tangents, you can tell the model what not to do or that “it’s okay to say ‘I don’t know.’” Anthropic’s internal tips include explicitly allowing the assistant to admit when information is missing (12 prompt engineering tips to boost Claude's output quality). Likewise, to maintain quality, you might impose format constraints (e.g. “answer in 3 bullet points”) or domain constraints (“focus on solutions relevant to healthcare”). Clear constraints guide the AI’s creativity into the useful range.

Self-Check Mechanisms: A cutting-edge prompting trick is to have the AI reflect or critique its own answer. After an initial answer, you can prompt it with something like, “Did we miss any important point?” or “Double-check the above and correct any gaps.” This leverages the model’s ability to iterate on its output and often leads to more complete results (Prompt Engineering Guide for UX/UI Designers | UXPin). Essentially, the AI can act as its own reviewer, catching omissions in a brainstorming list or adding an overlooked idea. This aligns with Anthropic’s suggestion to have the assistant review its response for completeness (Prompt Engineering Guide for UX/UI Designers | UXPin).

These principles lay the foundation for building powerful meta prompts. By combining clarity, role guidance, examples, stepwise thinking, and iterative refinement, we can significantly enhance AI-assisted brainstorming.
Optimized Meta Prompt Structures for Brainstorming
Using the above best practices, we can design meta prompt structures tailored for brainstorming. A meta prompt is a reusable template that is about 70% general structure and 30% tailored to the specific topic/domain. Below are three optimized prompt frameworks for AI brainstorming:
Variant 1: Open-Ended Divergent Brainstorming Prompt
Purpose: Encourage maximal creativity and a broad range of ideas. This structure is ideal for early-stage brainstorming when you want divergent thinking – generating as many ideas as possible without immediate filtering.

Structure: Begin with a brief role assignment emphasizing creativity, then clearly state the brainstorming topic or question. Invite the AI to produce a large number of ideas, explicitly telling it to defer judgment or criticism. You might also remind it to include unconventional or “wild” ideas. Finally, specify the desired format (e.g. a list of ideas). This prompt does not force a step-by-step analysis; it prioritizes free flow of ideas.

Key Features: Relies on clarity, role persona (e.g. “creative brainstormer”), and an open mandate for idea generation. It avoids constraining the model initially – no idea is “wrong” at this stage. (You can always follow up with evaluation prompts later.) The tone is encouraging and imaginative.

Example Template:

You are a highly creative brainstorming assistant. 

Brainstorm **as many ideas as possible** about **[Topic/Problem]**.  Think broadly and freely – even unconventional or wild ideas are welcome. Don't evaluate or filter the ideas yet; just list them.

Provide the ideas as a bullet-point list.

In this template, the text in bold and [brackets] would be replaced with the specific topic at hand. The general parts (role definition, instruction to think freely, listing format) make up ~70% of the prompt, while the specific topic is the remaining ~30%. This structure leverages the model’s open-ended creativity and is guided by the classic brainstorming rule of quantity over quality in the first pass.
Variant 2: Structured Step-by-Step Brainstorming Prompt
Purpose: Generate ideas in a systematic, stepwise fashion, ensuring thorough exploration of complex problems. This is useful when you want to organize the brainstorming output into categories or stages, or when the topic benefits from a logical breakdown.

Structure: The prompt instructs the AI to follow a sequence of steps. First, define the context and assign an expert role relevant to the topic (for instance, “You are an AI research assistant” if brainstorming research ideas). Then outline a structured approach – for example: (1) identify subtopics or key aspects of the problem, (2) brainstorm ideas for each aspect, and (3) conclude with a summary or best ideas. Essentially, the prompt itself contains a mini-plan that the AI should execute in order. This directly implements the “think step by step” advice (Prompt engineering for business performance \ Anthropic), as the AI will internally reason through each step in sequence.

Key Features: Clarity and specificity are crucial here – the prompt literally says what to do first, second, third, etc. The model is less likely to get off-track because it has a roadmap to follow. This structure often yields more organized output (e.g. ideas grouped by category or presented in a logical order). It uses chain-of-thought implicitly by breaking the task into sub-tasks within one prompt. As Gwern noted, some tasks become easier when subdivided (Methods of prompt programming :: — Moire), so this prompt helps the model handle complexity by portioning the work.

Example Template:

You are a systematic thinker and expert in **[Domain]**. Let’s brainstorm solutions for **[Specific Problem]** in a step-by-step manner.

1. **Breakdown:** First, list the main aspects or components of the problem that we should consider.

2. **Idea Generation:** Under each aspect, brainstorm a few creative solutions or ideas. Be specific and insightful.

3. **Summary:** Finally, highlight the most promising ideas from all those above (just the top 2-3 ideas overall, with a brief note on why they stand out).

Follow this structured format in your response, using numbered or bulleted lists as appropriate.

Here the prompt explicitly delineates the process (steps 1, 2, 3). The general structure (~70%) is the three-step framework which can be reused across topics, and the domain/problem (~30%) is plugged in. This variant ensures the AI “thinks” in stages and often produces a well-organized set of ideas, which is great for analysis or when you have a multifaceted challenge.
Variant 3: Role-Based Multi-Perspective Brainstorming Prompt
Purpose: Produce a diverse set of ideas by having the AI simulate multiple perspectives or experts. This structure is useful for complex decision-making or creative tasks where considering different viewpoints leads to richer brainstorming.

Structure: The prompt casts the AI in multiple roles at once or sequentially. For example, it might say: “You are a panel of experts” or “Put on the hat of different specialists and give their ideas.” Then it specifies the perspectives, such as different stakeholder roles, disciplines, or even fictional personas. The AI is asked to generate ideas from each perspective in turn. Finally, it may be asked to synthesize or compare those ideas. This approach draws on the insight that an LLM can simulate various characters or voices in text (The Waluigi Effect (mega-post) - LessWrong 2.0 viewer) – effectively, you get the benefit of a brainstorming group within a single AI.

Key Features: Strong use of persona prompting and context. By telling the model to act as, say, a marketer, a scientist, and a customer, each with their own viewpoint, we coax it to retrieve different subsets of its knowledge. This reduces the risk of one-dimensional answers. Janus’s simulator theory points out that language models can play many roles; giving a clear role prompt biases the output in that direction (The Waluigi Effect (mega-post) - LessWrong 2.0 viewer). In practice, this variant yields categorized ideas (by perspective) and ensures novelty because each “persona” will produce different angles. It’s like having a brainstorming session with varied experts, all generated by the AI.

Example Template:

You are a panel of experts brainstorming about **[Problem/Topic]**. Each expert will contribute ideas from their perspective:

- **Expert 1 (The [Perspective A]):** From this viewpoint, suggest some ideas or solutions.

- **Expert 2 (The [Perspective B]):** Now give ideas from another perspective.

- **Expert 3 (The [Perspective C]):** Contribute ideas from this angle as well.

After listing ideas from each perspective, **conclude with a combined list** of the most compelling ideas overall (avoiding duplicates).

In this template, the general scaffold defines a multi-expert brainstorm and how the answer should be structured by role. The specific parts are the problem/topic and the named perspectives to use (which you would fill in according to your domain). The general instructions and format are ~70% of the text, while the choices of roles and topic are the 30% that make it domain-specific. By using this method, the AI will effectively “play” each role and generate ideas accordingly – for instance, a conservative viewpoint might raise practical considerations while an imaginative viewpoint adds wild ideas, covering a broad spectrum when combined.

Each of these three meta prompt structures can guide an AI to produce rich brainstorming output. Next, we’ll see how to decide which structure to use and how to adapt them to different scenarios.
Decision Matrix: Choosing the Right Prompt Variant
Different brainstorming situations call for different prompting strategies. Below is a simple decision matrix to help determine which of the three prompt variants is most useful given the context and goals:

Prompt Structure
Best Used When...
Open-Ended Divergent 

(Variant 1)
Goal: Maximum creativity and volume of ideas. 

**Use when:** You are in an early ideation phase, need lots of diverse suggestions, or want to explore an unbounded problem space. This variant shines for *creative writing prompts or broad problem exploration* where novelty is valued over structure. It’s less suited for highly complex problems that need analysis, but great for breaking past writer’s block or generating out-of-the-box concepts.
Structured Step-by-Step 

(Variant 2)
Goal: Thorough, organized idea development. 

**Use when:** The problem is complex or multifaceted, and you need to ensure coverage of all parts of the problem. Ideal for analytical or technical brainstorms (e.g. scientific research planning, product design with multiple requirements) where a systematic approach prevents oversight. This is also useful when you plan to refine or implement ideas later, since the output will already be categorized and justified stepwise.
Role-Based Multi-Perspective 

(Variant 3)
Goal: Diversity of viewpoints and comprehensive insight. 

**Use when:** You want to avoid echo-chamber thinking and ensure **varied perspectives** on the topic. Great for business strategy (considering marketing vs. engineering angles, etc.), interdisciplinary problems, or any scenario where balancing different priorities is key. If an issue could benefit from expert opinions or stakeholder input, this prompt yields ideas covering those different concerns. It can handle complexity too, but its strength is in breadth of perspective rather than stepwise depth.


How to use the matrix: Identify the nature of your brainstorming task (is it open-ended creative? multi-part and complex? needing multiple viewpoints?). Match it to the variant that emphasizes what you need. In some cases, you might even combine approaches – for example, use a role-based prompt (variant 3) and within each role's response, encourage stepwise thinking (variant 2) if you need both breadth and depth. The decision matrix helps ensure you start with the most fitting meta prompt to get quality results efficiently.
Adapting the Meta Prompt for Other Use Cases (70/30 General vs. Specific)
The meta prompts above are designed to be adaptable. The idea is to maintain about 70% of the prompt as general structure (the part that encodes the best practices and approach), while 30% is customized to the domain or specific task. Here are general principles for modifying a meta prompt to other use cases while keeping that balance:

Keep the Core Instructions Constant: The general process – whether it’s free brainstorming, stepwise thinking, or multi-role format – remains the same across domains. This core (e.g. “First do X, then do Y” or “You are a panel of experts…”) should be written in a domain-agnostic way. It forms roughly 70% of the prompt and ensures consistency. For example, the instruction “generate as many ideas as possible without filtering” works for any topic.

Swap in Domain Details for Specificity: The remaining ~30% of the prompt should be tailored to the new context. This includes inserting the specific topic/problem statement, any relevant domain context, and the particular roles or examples that fit that field. Essentially, you fill in the blanks of the template. For instance, in a medical research brainstorm, you’d specify the medical problem and perhaps roles like “clinician” or “biologist” for variant 3. In a marketing campaign brainstorm, you’d plug in the product or audience details. These specifics guide the AI to generate content relevant to your scenario while the overall prompt structure guides how to generate it.

Use Domain Vocabulary and Constraints: In the domain-specific portion, incorporate key terminology or constraints of that field. This makes the prompt resonate more with the model’s knowledge of the domain. For example, for a software brainstorming prompt, you might mention words like “user interface,” “backend”, or specific tech constraints; for a creative writing prompt, you might mention “character, setting, conflict” to anchor the discussion. Adding such details helps the AI ground its ideas in the appropriate context.

Maintain a Consistent Tone and Style: Ensure the general instructions and the domain specifics mesh in tone. If your general prompt is phrased formally, word the specific details formally as well. If the domain is playful (e.g. game design), you can allow a more playful tone. The 70/30 split is about content, but style should be coherent throughout. The general structure might even include a style guideline like “answer in a professional tone” which you would keep consistent across adaptations.

Check the 70/30 Balance: After drafting a prompt for a new use case, review it and roughly estimate: is the majority of this prompt the reusable template, and did I only customize the necessary parts? For example, if you find yourself rewriting the whole approach for a new domain, you may be deviating from the meta prompt concept. In most cases, you only change the topic description, a few keywords, and perhaps the roles/examples – not the fundamental prompt logic. Maintaining this balance ensures you’re leveraging proven prompt scaffolds while still targeting the AI to the domain.

Example of 70/30 Adaptation: Suppose variant 2’s template (structured steps) is being adapted from a product design context to an environmental policy context. The general steps “Breakdown – Idea Generation – Summary” remain (that’s the 70%). What changes (30%) is the problem statement and possibly the framing of steps: “main aspects of the environmental issue” instead of “aspects of the product design,” etc. The adapted prompt might also include domain-specific criteria in the summary (e.g. “highlight the most sustainable ideas”). But it would not change the fact that there are three steps or the requirement of listing and then summarizing – those parts come straight from the general template.

By following these principles, you can confidently reuse each meta prompt structure for virtually any topic – whether you’re brainstorming a new marketing strategy, plotting a novel, or devising a research plan – with minimal changes. The general portion ensures the AI follows a reliable process, and the specific portion makes the content relevant to your particular needs.
Examples of Variations in Different Domains
To illustrate how these prompt strategies carry over to various fields, below are brief examples applying the meta prompt variations to three domains: business strategy, creative writing, and scientific research. In each case, notice how the general structure stays consistent while the domain-specific details are plugged in.
Example 1: Business Strategy Brainstorming (Role-Based Multi-Perspective)
Let’s use Variant 3 (Multi-Perspective) for a business scenario. Suppose we want to brainstorm ways to improve a company’s remote work policy. We’ll assign a few roles relevant to a business context.

You are a panel of experts brainstorming how to improve **remote work productivity** at a tech company.

- **Expert 1 (HR Manager):** Suggest ideas focused on employee engagement, communication, and well-being.

- **Expert 2 (CTO/IT Specialist):** Propose ideas involving tools or technologies to enhance remote collaboration.

- **Expert 3 (Project Manager):** From a workflow perspective, suggest process changes or best practices for remote teams.

After each expert’s ideas, compile a shortlist of the most impactful suggestions that the company should consider implementing.

Why it works: The structure is the role-based template – we kept the panel format and the instruction to compile a shortlist (general part). We filled in specific roles (HR Manager, CTO, Project Manager) and the topic (remote work productivity). The AI, following this prompt, would produce ideas from each expert’s viewpoint (e.g. HR Manager might suggest virtual team-building events, the CTO might suggest better VPN and collaboration software, etc.), then summarize top ideas. This approach ensures we cover HR, tech, and project management angles of the problem, yielding a well-rounded set of strategic ideas.
Example 2: Creative Writing Brainstorming (Open-Ended Divergent)
For a creative writing domain, we’ll use Variant 1 (Open-Ended) to generate a bunch of story ideas. Imagine we want ideas for a fantasy novel setting.

You are a wildly imaginative writing muse. 

Brainstorm as many unique **fantasy world ideas** as possible for a new novel. The ideas can range from magical forests to entire invented universes – feel free to be unconventional. Do **not** critique the ideas; just let them flow.

List each idea as a separate bullet point.

Why it works: This prompt sticks to the open-ended format. The general portion (role as “imaginative muse,” instruction to brainstorm freely without filtering, listing format) stays the same regardless of genre. The specific part here is just “fantasy world ideas for a new novel.” As a result, the AI might output a list like: “- A floating island civilization powered by dragon magic,” “- A post-apocalyptic fairy kingdom where technology and magic intertwine,” and so on – a flurry of creative premises. Because we explicitly said not to critique or limit the ideas, we encourage a free flow of creativity, which is exactly what a writer needs in early brainstorming. Later, the writer could take this list and ask the AI (or themselves) to expand or refine the favorites, but Variant 1 serves its purpose by providing a wealth of raw material.
Example 3: Scientific Research Brainstorming (Structured Step-by-Step)
Now for a scientific research context, we apply Variant 2 (Step-by-Step). Suppose the task is brainstorming experiment ideas to increase crop resilience to drought.

You are an AI research assistant with expertise in agriculture. Let’s brainstorm **experimental approaches to improve drought resistance in crops**.

1. **Key Factors:** First, list the main factors or angles scientists might investigate (for example, genetic modification, soil treatment, water usage techniques, etc.).

2. **Potential Experiments:** For each factor above, suggest one or two specific experiments or study ideas that could be done to enhance drought resistance. Be detailed – what would the experiment involve measuring or comparing?

3. **Analysis:** Finally, briefly note any challenges or novel insights for each proposed experiment (e.g. potential difficulties in field testing, or why an idea could be especially promising).

Please present the ideas in a structured list organized by the factors identified.

Why it works: We’ve used the structured variant template almost verbatim, just tailoring it to the subject of crop drought resistance. The general scaffold (identify key factors → propose experiments for each → comment on challenges/insights) guides the AI to produce an organized answer. The domain-specific inserts are the context (agriculture, drought resistance) and examples of what factors might be (to help the AI along). Following this prompt, the AI might list factors like plant genetics, irrigation methods, soil microbes, etc. under step 1; then under each, give experiment ideas (e.g. a gene editing trial for a drought-tolerance gene, a controlled irrigation schedule experiment, introducing certain microbes to soil plots, etc.), each with a short note of analysis. The output would be neatly structured and highly relevant to scientific research brainstorming – essentially a mini research proposal outline. This shows how the stepwise template can be applied to an academic domain to ensure thoroughness and clarity.
